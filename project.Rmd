---
title: "STAT 331 Project"
author: "SiJie Jin, Yinong Wang, Raymond Tan"
date: "November 14th, 2018"
output: pdf_document
toc: yes
toc_section: 2
theme: united
highlighted: tango
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mice)
```
\newpage
# Summary

The objective of this project is to explore and analyze the model that best describes the relation between the weight of healthy male single-fetus baby and some possible factors.

We firstly cleaned and categorized data in a property way that reduces the fraction of misleading results. Then, from pair plot, we concluded a pre-fitting model and, using automated model selection techniques, we obtained another three models. We then used manual model selection methods including PRESS, $R^2$, AIC and narrowed the number of our candidate models down to two. Then we further diagonsed the two models by checking their linear assumptions, leverage, and outliers and also by runnning cross validations. Finally, we drew conclusions on the best model we obtained and hence found a reasonably good relation between between the healthy male single-fetus birth weight and some possible factors.

Based on the analysis, the model comming out from stepwise automated model selection best describes this relation. According to this model, we propose that gestation, ethnicity of mother, mother height, mother weight, father height, father weight, when mother quit smoke before pregnacy and the interaction between gestation and income are some factors that have significant effects on the birth weight.

# Model Selection


## Pre-fitting data diagnostic

```{r prefit ,echo = FALSE}
# read in data
births.raw = read.csv("chds_births.csv")
```

### Summary of Data

After loading the dataset, we observed the following:

1. There are 1236 entries in the dataset
2. There are 18 factors in the dataset
3. Some factors have a large proportion of NA's:
    + fht: 39.81%
    + fwt: 40.37%
    + income: 10.03%
4. Factor marital contains undefined level 0



Table of proportions of NA for each covariate:
```{r,echo=FALSE}
# generate proportion of NA's in each covariate
proportion <- sapply(births.raw, function(x) { sum(is.na(x))*100/nrow(births.raw) })
proportion
```



Summary of column marital in the dataset:
```{r,echo=FALSE}
summary(births.raw$marital)
```


```{r,echo=FALSE}
# Validating Variables
births.raw$marital[!births.raw$marital %in% c(1:5)] = NA

# Setting unknown data to NA so that they can be imputed
births.raw$time[births.raw$time %in% c(9)] = NA
births.raw$number[births.raw$number %in% c(9)] = NA
```

### Categorize data
By using one hot encoding method, we factorize the levels of each categorical variables into meaningful names to have a good interpretation of the model.

* Note: we also shrink the number of levels for some factors because those levels are significant minorities:
    + meth/feth: Mexican, Asin, and Mixed are significant minorities and are merged into level Other
    + med/fed: elementary school, high school + trade school, trade school, and high school unclear are significant minorities and are merged into level Other
    + marital: legally separated, divorced, widowed, and never married are significant minorities and are merged into level single
    + time: less than a year, 1-2 years, 2-3 years, 3-4 years, 5-9 years are significant minorities and are merged into level less than 10 years; never and more than 10 years are significant minorities and are merged into level never or more than 10 years
    + number: 1-4, 5-9, 10-14, 15-19 are significant minorities and are merged into level less than 20; 20-29, 30-39, 40-60, more than 60 are significant minorities and are merged into level more than 20
* Note: we converted the following factor levels into NA for mice to impute more useful data
    + time: level quit but don't know when is converted into NA
    + number: level smoked but don’t know how much is converted into NA
* Note: the level never smoked of factor smoke is identical to the level never smoked of factor time and number, so we droped this factor

```{r,echo=FALSE}
# list of categorical variables
categoricals = c('meth', 'feth', 'med', 'fed', 'marital',
                 'income', 'smoke', 'time', 'number')#, 'parity', 'mht', 'fht')
# list of continuous variables
continuous = names(births.raw)[! names(births.raw) %in% categoricals]

# Categorize m/feth
meth.categories = c(rep('Caucasian', 6), 'Other', 
                    'African-American', 'Other', 'Other', 'Other')
for (i in 1:length(meth.categories)) {
  births.raw$meth[births.raw$meth == i-1] = meth.categories[i]
  births.raw$feth[births.raw$feth == i-1] = meth.categories[i]
}
births.raw$meth = as.factor(births.raw$meth)
births.raw$feth = as.factor(births.raw$feth)

# Categorize m/fed
med.categories = c('Other', 'middle school', 'high school', 'Other',
                   'high school + some college', 'college graduate',
                   'Other', 'Other')
for (i in 1:length(med.categories)) {
  births.raw$med[births.raw$med == i-1] = med.categories[i]
  births.raw$fed[births.raw$fed == i-1] = med.categories[i]
}
births.raw$med = as.factor(births.raw$med)
births.raw$fed = as.factor(births.raw$fed)

# Categorize marital
marital.categories = c('married', 'single', 'single', 
                       'single', 'single')
for (i in 1:length(marital.categories)) {
  births.raw$marital[births.raw$marital == i] = marital.categories[i]
}
births.raw$marital = as.factor(births.raw$marital)

# Categorize income
income.categories = c('under 2500', '2500-4999', '5000-7499','7500-9999', 
                      '10000-12499', '12500-14999', '15000-17499', 
                      '17500-19999', '20000-22499', 'over 22500')
for (i in 1:length(income.categories)) {
  births.raw$income[births.raw$income == i-1] = income.categories[i]
}
births.raw$income = as.factor(births.raw$income)

# Categorize smoke
smoke.categories = c('never', 'smokes now', 'until pregnancy', 
                     'used to, not anymore')
for (i in 1:length(smoke.categories)) {
  births.raw$smoke[births.raw$smoke == i-1] = smoke.categories[i]
}
births.raw$smoke = as.factor(births.raw$smoke)

# Categorize time
time.categories = c('never or more than 10 years', 'still smokes', 
                    'during pregnancy', 'less than 10 year', 
                    'less than 10 year', 'less than 10 year', 
                    'less than 10 year', 'less than 10 year', 
                    'never or more than 10 years', 'quit but don’t know when')
for (i in 1:length(time.categories)) {
  births.raw$time[births.raw$time == i-1] = time.categories[i]
}
births.raw$time = as.factor(births.raw$time)

# Categorize number
number.categories = c('never smoked', 'less than 20', 'less than 20', 
                      'less than 20', 'less than 20', 'more than 20',
                      'more than 20', 'more than 20', 'more than 20', 
                      'smoked but don’t know how much')
for (i in 1:length(number.categories)) {
  births.raw$number[births.raw$number == i-1] = number.categories[i]
}
births.raw$number = as.factor(births.raw$number)

# delete covariate smoke
births.raw$smoke = NULL
```

### Pair plots

```{r,echo=FALSE}
# reduce the dataset to contain only continuous data
births.raw.cont = births.raw[ , ! names(births.raw) %in% categoricals]

pairs(births.raw.cont, cex = 0.2)
```
\newline
Here is the pair plots of continuous variables. Notice that $parity$ is not continuous because there are some vertical lines in the plot. Also $parity$ is the total number of previous pregnancies, so normally it does not contain decimal numbers. What we did was categorizing it into 6 different variables.

```{r,echo=FALSE}
# Categorize parity
births.raw$parity[births.raw$parity > 5] = 5
parity.categories = c('zero', 'one', 'two', 'three', 'four', 'more than four')
for (i in 1:length(parity.categories)) {
  births.raw$parity[births.raw$parity == i-1] = parity.categories[i]
}
births.raw$parity = as.factor(births.raw$parity)

table(births.raw$parity)
```


### NA's and $\pm$Inf's

For choosing the best 2 canadiate models, we want AIC and Press to be as small as possible and $R^2$ to be as large as possible when comparing. From the data above, we can conclude that father height$(fht)$, father weight$(fwt)$, $marital, number, time$ and $income$ are missing covariates since these variables take up to a large proportion over 10% of our dataset, which might give us undsired results when modelling these data. Since throwing out more than 10% of our data could cause misleading to our analysis. We would compelete these covariates by using Imputatation 5 times via the mice package.


```{r,echo=FALSE,warning=FALSE}
# impute the data via mice package
invisible(capture.output(births.mice <- mice(births.raw, method = "cart", seed = 1)))
births.clean <- complete(births.mice)
```

## Pre-fitting Model

```{r,echo=FALSE}
# Variance Inflaction Factor
# design matrix excluding intercept
X <- model.matrix(lm(wt ~ gestation + mage + mwt + fage
                     + fwt + mht + fht - 1, data = births.clean))
C <- cor(X) # correlation matrix
vif <- diag(solve(C))

# pre-fitting model
MPrefit <- lm(wt ~ gestation + mage + mwt + fwt + fht + mht, data = births.clean)
```

We got a pre-fitting model $MPrefit$ by analyzing pair plots of continuous variable. We noticed that there is a strong linear relation between mother age$(mage)$ and father age$(fage)$. By analyzing variance inflaction factor, father age$(fage)$ has more correlated relation with other variables. Hence in the pre-fitting model, we decided to remove the factor father age$(fage)$.
$$MPrefit: wt \sim gestation + mage + mwt + fwt + fht + mht$$

## Automated Model Selection

```{r,echo=FALSE}
M0 <- lm(wt ~ 1, data = births.clean) # initial model
Mfull <- lm(wt ~ ( . - number - time - fed - med - feth - meth - marital)^2+
              number + time + fed + med + feth + meth + marital +
              I(gestation^2) + I(mage^2) + I(mht^2) + I(mwt^2) + I(fage^2) + 
              I(fht^2) + I(fwt^2), data = births.clean) # full model
Mstart <- lm(wt ~ ., data = births.clean) # start model
```

We defined three helper model $M0$, $Mfull$, and $Mstep$. 
Model $M0$ is a model containing solely the intercept. $$M0: wt \sim 1$$
$Mfull$ is a model containing most the main effects and interactions. When we first tried backward elimination, we used the full model with all the main effects and interactions. However it took us a long time to find a suitable model because there was a large number of categorical variables in our full model. We deleted some non-interaction terms and added quadratic terms for the continuous covariates. Here is our $MFull$: $$Mfull: wt \sim ( . - number - time - fed - med - feth - meth - marital)^2+$$
              $$number + time + fed + med + feth + meth + $$
              $$marital + I(gestation^2) + I(mage^2) + I(mht^2) + $$
              $$I(mwt^2) + I(fage^2) + I(fht^2) + I(fwt^2)$$
$Mstep$ is a model what contains only the main covariate. $$Mstep: wt \sim .$$

```{r,echo=FALSE,eval=FALSE}
# detect model coefficients which are NA
beta.max <- coef(Mfull)
names(beta.max)[is.na(beta.max)]
```

```{r,echo=FALSE}
# forward selection
Mfwd <- step(object = M0,
             scope = list(lower = M0, upper = Mfull),
             direction = "forward",
             trace = FALSE)
```

Using forward selection, we obtained a model named $Mfwd$. This model starts with $M0$ and then keep adding the "best" factors so far onto the model without exceeding the complexity of $Mfull$
$$Mfwd: wt \sim gestation + time + I(mht^2) + I(fwt^2) + meth + parity + $$
$$number + mwt + I(fht^2) + mht$$

```{r,echo=FALSE}
# backward elimination
Mback <- step(object = Mfull,
              scope = list(lower = M0, upper = Mfull),
              direction = "backward",
              trace = FALSE)
```

Using backward selection, we obtained a model named $Mback$. This model starts with $Mfull$ and keep deducting factors that are least significant.
$$Mback: wt \sim gestation + parity + mage + mht + mwt + fage + fht + fwt + $$
    $$income + number + time + meth + I(gestation^2) + $$
    $$I(mht^2) + I(mwt^2) + I(fage^2) + gestation:parity + $$
    $$gestation:mage + gestation:mht + gestation:fwt + $$
    $$parity:mage + parity:fage + mage:mwt + mage:fage + $$
    $$mage:fht + mht:fage + mht:fwt + mht:income + $$
    $$mwt:fht + fht:fwt$$
    
```{r,echo=FALSE}
# stepwise selection
Mstep <- step(object = Mstart,
              scope = list(lower = M0, upper = Mfull),
              direction = "both",
              trace = FALSE)
```

Using stepwise selection, we obtained a model named $Mstep$. This model starts with $Mstart$ and adds or drops factors accordingly.
$$Mstep:  wt ~ gestation + parity + meth + mht + mwt + fht + fwt + income + $$
    $$time + number + I(mht^2) + I(gestation^2) + $$
    $$gestation:income + mht:fwt + fht:fwt + $$
    $$mht:income + gestation:fwt + gestation:parity$$

## Manual Model Selection

```{r,echo=FALSE,eval=FALSE}
# exam three automatically selected models
summary(Mfwd)
summary(Mback)
summary(Mstep)
```

Now we have four candidate models, we use PRESS, $R^2$, and AIC to manually further reduce our candicate models down to two.

### PRESS, $R^2$ and AIC

```{r,echo=FALSE}
# Press Statistic
calPress <- function(M){
  press <- resid(M)/(1-hatvalues(M))
  press <- sum(press^2)
  return(press)
}

# Calculate PRESS for all four models
press1 <- calPress(Mfwd);
press2 <- calPress(Mback);
press3 <- calPress(Mstep);
press4 <- calPress(MPrefit)

presses <- c(press1, press2, press3, press4)

# AIC Statistic
# Calculate AIC for all four models
AIC1 <- AIC(Mfwd)
AIC2 <- AIC(Mback)
AIC3 <- AIC(Mstep)
AIC4 <- AIC(MPrefit)

AICS <- c(AIC1, AIC2, AIC3, AIC4)

# R^2
# Calculate R^2 for all four models
r.squared <- c(summary(Mfwd)$r.squared, summary(Mback)$r.square, 
               summary(Mstep)$r.square, summary(MPrefit)$r.square)

# Make table of all informations for printing
info <- c(r.squared,AICS,presses)
info <- matrix(t(info),nrow = 4,ncol = 3)
rownames(info)<-c("MFwd", "MBack", "MStep", "MPrefit")
colnames(info)<-c("R^2", "AIC", "Press")
info
```

We got 3 different models, $MFwd$, $MBack$, $MStep$, by performing forward selection, backward elimination and stepwise selection on full model respectively. From the above table generated by R, we can see that $MBack$ has the smallest AIC Press Statistic and has the largest $R^2$ compared to the other two models. By comparing $MStep$ and $MFwd$, $MStep$ seems to have a better $R^2$ and a better AIC than $MFwd$, but $MFwd$ only has a better PRESS Statistic than $MStep$. Therefore, $Mback \cap MStep \in$ our canadiate set. 
We also got a pre-fitting model $MPrefit$. Since $MPrefit$ has least $R^2$ and largest AIC and PRESS Statistic, We do not choose this model as our canadiate model.

```{r,echo=FALSE}
# models to compare
M1 <- Mback
M2 <- Mstep
```

# Model Diagnostics

## Linear Model Assumptions

Since our canadiate set only has $MBack$ and $MStep$, we only need to generate plots,Studentized Residuals VS Fitted, Standardized Histogram and QQ plot, for these individuals.

```{r,echo=FALSE}
# function for drawing studendized residual plots
StuResPlot <- function(M,name) {
  cex <- .8
  res <- residuals(M)
  h <- hatvalues(M)  # Hat matrix
  res.stu <- res/sqrt(1-h) # studentized residuals, but on the data scale
  y.hat <- predict(M)
  par(4,.1,4,.1)
  plot(y.hat, res, pch = 21, bg = "black", 
       cex = cex, 
       cex.axis = cex,
       xlab = "Predicted Birth Weight",
       ylab = "Residual Birth Weight",
       main = paste("Studentized Residuals",name),
       abline(h = mean(res), col = "blue"))
  points(y.hat, res.stu, pch = 21, bg = "red", cex = cex)
  legend(x = "bottomleft", c("Residuals", "Studentized Residuals"),
         pch = 21, pt.bg = c("black", "red"), pt.cex = cex, cex = cex)
}

# function for drawing standardized residual plots
StaResPlot <- function(M,name) {
  cex <- .8
  res <- residuals(M) # Hat matrix
  h <- hatvalues(M)
  sigma.hat <- sigma(M)
  par(4,.1,4,.1)
  hist(res/sigma.hat, breaks = 50, freq = FALSE, cex.axis = cex,
  xlab = "Standardized Residual", main = paste("Standardize Residuals",name))
  curve(dnorm(x), col = "red", add = TRUE)
  abline(v = mean(res/sigma.hat), col = "green")
}

#function for drawing QQ plots
QQPlot <- function(M,name) {
  res <- residuals(M)
  h <- hatvalues(M)
  qqnorm(res/sigma(M),main = paste("QQ Plot",name))
  abline(a=0, b=1, col = "red") #add 45 degree line
}

par(mfrow = c(1,2))

# studendized residual for backward elimination model:
StuResPlot(Mback,"(Mback)")

# studentdized residual for stepwise selection model:
StuResPlot(Mstep,"(MStep)")

```

To begin with, we compare Studentized Residuals plot between the two canadiates. These two plots are very similar in a way such that their mean at zero(shown by a blue horizontal line), they do not display any linear/cureventure trends, which impiles there they are independence, they both have a constant variance since the error variance does not have a increasing/decresing trend with the independent varaibles.



```{r,echo=FALSE}
par(mfrow = c(1,2))

# standardized residual for backward elimination model:
StaResPlot(Mback,"(MBack)")

# standardized residual for stepwise selection model:
StaResPlot(Mstep,"(MStep)")
```


Next, we would look at the Standardized Historgram to find the assumption of mormality and to find any abnormal residuals about the rwo models. From the graphs, $Mstep$ has a slightly better normality than $MBack$. As the density of $Mstep$ touches the normal curve in range of [[-3,-1],[1,3]], whereas the density of $MBack$ touches the normal curve in the range of [[-2,-3],[2,3]]. In additon, we can clearly see that there is no standardized residuals lies outside of [-3,3], which impiles there is no abnormal residuals for both models.

```{r,echo=FALSE}
par(mfrow = c(1,2))

# QQ-plot for backward selection model:
QQPlot(Mback,"(MBack)")

# QQ-plot for stepwise selection model:
QQPlot(Mstep,"(MStep)")
```

Last but not least, We compare their QQ-Plots. From both QQ-Plots, as theroretical quantiles getting larger, each points tend to stabalize on the 45 degree line on the plots.\newline

In Conclution, we can not determine which model is better than the other from a linear assumptions point of view since the evidences from above can not distinguish the advantage of the two. 

## Leverage and Influence
```{r,echo=FALSE}
# function to produce leverage and influence plots
LeverageInfluence <- function(M) {
  y.hat <- predict(M) # predicted values
  sigma.hat <- sigma(M)
  res <- resid(M) # original residuals$
  res.sta <- res/sigma.hat # standardized residuals

  # compute leverages
  h <- hatvalues(M) # hat matrix
  res.stu <- res.sta/sqrt(1-h) # studentized residuals

  # PRESS residuals
  press <- res/(1-h)

  # DFFITS residuals
  dfts <- dffits(M)

  # standardize each of these such that they are identical at the average
  # leverage value
  p <- length(coef(M))
  n <- nobs(M)
  hbar <- p/n # average leverage
  res.stu <- res.stu*sqrt(1 - hbar) # at h = hbar, res.stu = res.sta
  press <- press*(1 - hbar)/sigma.hat # at h = hbar, press = res.sta
  dfts <- dfts*(1 - hbar)/sqrt(hbar) # at h = hbar, dfts = res.sta

  # plot all residuals
  par(mfrow = c(1,3), mar = c(4,4,.1,1.1))
  cex <- .5
  plot(y.hat, rep(0, length(y.hat)),
       type = "n", # empty plot to get the axis range
       ylim = range(res.sta, res.stu, press, dfts),
       cex.axis = cex,
       xlab = "Predicted Values",
       ylab = "Residuals")

  # dotted line connecting each observations residuals for better visibility
  segments(x0 = y.hat,
           y0 = pmin(res.sta, res.stu, press, dfts),
           y1 = pmax(res.sta, res.stu, press, dfts),
           lty = 2)
  points(y.hat, res.sta, pch = 21, bg = "black", cex = cex)
  points(y.hat, res.stu, pch = 21, bg = "blue", cex = cex)
  points(y.hat, press, pch = 21, bg = "red", cex = cex)
  points(y.hat, dfts, pch = 21, bg = "orange", cex = cex)

  # against leverages
  plot(h, rep(0, length(y.hat)),
       type = "n",
       cex.axis = cex,
       ylim = range(res.sta, res.stu, press, dfts),
       xlab = "Leverages", ylab = "Residuals")

  segments(x0 = h,
          y0 = pmin(res.sta, res.stu, press, dfts),
          y1 = pmax(res.sta, res.stu, press, dfts),
          lty = 2)
  points(h, res.sta, pch = 21, bg = "black", cex = cex)
  points(h, res.stu, pch = 21, bg = "blue", cex = cex)
  points(h, press, pch = 21, bg = "red", cex = cex)
  
  points(h, dfts, pch = 21, bg = "orange", cex = cex)
  abline(v = hbar, col = "grey60", lty = 2)

  # cook's distance vs. leverage
  D <- cooks.distance(M)

  # flag some of the points
  infl.ind <- which.max(D) # top influence point
  lev.ind <- h > 2*hbar # leverage more than 2x the average
  clrs <- rep("black", len = n)
  clrs[lev.ind] <- "blue"
  clrs[infl.ind] <- "red"
  # par(mfrow = c(1,1), mar = c(4,4,1,1))
  cex <- .8
  plot(h, D, xlab = "Leverage", ylab = "Cook's Influence Measure",
  pch = 21, bg = clrs, cex = cex, cex.axis = cex)

  
  p <- length(coef(M))
  n <- nrow(births.clean)
  hbar <- p/n # average leverage
  abline(v = 2*hbar, col = "grey60", lty = 2) # 2x average leverage
  legend("topleft", legend = c("High Leverage", "High Influence"), pch = 21,
         pt.bg = c("blue", "red"), cex = cex, pt.cex = cex)
  
  return(c(infl.ind, D[infl.ind]))
}

# calculate leverage influence:
a <-LeverageInfluence(Mback)
b <-LeverageInfluence(Mstep)
```
The plots above display the residuals vs. fitted values using four types of residuals: standardized, studentized, PRESS, and DFFITS. The different types of residuals have been standardized such that each of them are identical at the average leverage value.
To measure the overall influence of each observation, Cook’s distance is plotted against leverage as above. We can see that $MStep$ has larger leverage and influence. Hence there might be an outlier in $MStep$.

## Outliers
```{r,echo=FALSE}
plot(predict(M2), resid(M2))
entry <- which.max(abs(resid(M2)))
```
We found that observation 240 had the largest residual in $Mstep$ and so it is the outlier of $MStep$. We found that birth weight in this observation is much larger than birth weight in other observations. This might result this observation be an outlier.

## Cross Validation
```{r,echo=FALSE,warning=FALSE}
# Cross-validation setup
nreps <- 2e3 # number of replications
ntot <- nrow(births.clean) # total number of observations
ntrain <- floor(ntot * 0.7) # size of training set
ntest <- ntot-ntrain # size of test set
mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
mspe2 <- rep(NA, nreps)
logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
for(ii in 1:nreps) {
  # randomly select training observations
  train.ind <- sample(ntot, ntrain) # training observations
  # refit the models on the subset of training data; ?update for details!
  M1.cv <- update(M1, subset = train.ind)
  M2.cv <- update(M2, subset = train.ind)
  # out-of-sample residuals for both models
  # that is, testing data - predictions with training parameters
  M1.res <- births.clean$wt[-train.ind] -
            predict(M1.cv, newdata = births.clean[-train.ind,])
  M2.res <- births.clean$wt[-train.ind] -
            predict(M2.cv, newdata = births.clean[-train.ind,])
  # mean-square prediction errors
  mspe1[ii] <- mean(M1.res^2)
  mspe2[ii] <- mean(M2.res^2)
  # out-of-sample likelihood ratio
  M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) # MLE of sigma
  M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
  # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
  logLambda[ii] <- sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log = TRUE))
  logLambda[ii] <- logLambda[ii] -
                   sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log = TRUE))
}
```

```{r,echo=FALSE}
# plot rMSPE and out-of-sample log(Lambda)
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = expression(M[BACK], M[STEP]), cex = .7,
        ylab = expression(sqrt(MSPE)), col = c("yellow", "orange"))
hist(logLambda, breaks = 50, freq = FALSE,
     xlab = expression(Lambda^{test}),
     main = "", cex = .7)
abline(v = mean(logLambda), col = "red") # average value
```
\newline
From the MSPE graph, we can observe that the mean square prediction error for both $Mback$ and $Mstep$ are almost identical.

From the out-of-sample likelihood ratio statistic, we can observe that there is a slight preference for Mstep.

# Conclusions

Q1. What are the most important factors associated with/influencing birth weight?
To find out what are the most significance factors associated with birth weight, we can look at the p-values for each betas. 
```{r,echo = FALSE}
extractP <- function(index) {
  anova(Mstep)$'Pr(>F)'[index]
}

constructP <- function(x) {
  pVal <- c()
  for(i in 1:length(x)){
    pVal <- c(pVal,extractP(x[i]))
  }
  
  return(pVal)
}

x <- c(1,3,4,5,6,7,9,13)
var <- c("gestation ","meth","mht","mwt","fht","fwt","time","gestation:income")
info.pVal <- rbind(var,constructP(x))
info.pVal






```
Here is a list of important factors extracted from the p-value column of the anova table generate by R. The list with the samllest p-value includes gestation, ethnicity of mother, mother height, mother weight, father height, father weight, when mother quit smoke before pregnacy and gestation interacts with income.  



Q2. Low birth weight is considered to be 88 ounces or less. Based on this analysis, would you be able to recommend behavioral changes to parents in order to avoid low birth weight? If so, please carefully formulate your recommendation.

At 0.05 significance level, notice that gestation, parity, ethnicity of mother, mother height, mother weight, father height, father weight, when mother quit smoke before pregnacy, number of cigarettes mother smokes per day when she does, interaction between gestation and income, interaction between mother height and father weight, interaction between mother height and income are significant factors that affect the birth weights. Hence parents need to pay attention to those factors to avoid a healthy birth weight. Some recommendations may be:
* the mother should reduce the number of cigarettes mother smoked per day if the mother does smoke because the p-value < 0.05 and we do reject the null hypothesis that the number of cigarettes mother smoked per day is insignificant to the birth weight at 0.05 significant level
* the mother should quit smoking a longer time prior to pregency because the p-value < 0.05 and we do reject the null hypothesis that the when mother quit smoking prior pregency is insignificant to the birth weight at 0.05 significant level
* both parents should try to gain weight because the p-value < 0.05 and we do reject the null hypothesis that either of the parents' weight is insignificant to the birth weight at 0.05 significant level



Q3. Are there any coefficients with high p-values retained in the final model? If so, why?

Q4. Are there any outlying observations that might be appropriate to remove?

```{r,echo=FALSE,results="hide"}


```

Q5. Are any of the regression assumptions of the final model violated? If so, which ones? What are the possible deficiencies of the final model? How do these deficiencies nuance your conclusions/recommendations above?




\newpage
#Appendix for R Code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```




