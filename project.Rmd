†---
title: "STAT331 Project"
author: "Sijie Jin, Raymond Tan, Yinong Wang"
date: '2018-11-14'
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

# Model Selection

## Pre-fitting data diagnostic

```{r}
library(mice)

# read in data
births.raw = read.csv("chds_births.csv")
```

### Summary of Data

```{r}
head(births.raw, 10)
summary(births.raw)

# Validating marital
births.raw$marital[!births.raw$marital %in% c(1:5)] = NA
births.raw$time[births.raw$time %in% c(9)] = NA
births.raw$number[births.raw$number %in% c(9)] = NA
```

### Categorize data

```{r}
categoricals = c('meth', 'feth', 'med', 'fed', 'marital', 'income', 'smoke', 'time', 'number')#, 'parity', 'mht', 'fht')
continuous = names(births.raw)[! names(births.raw) %in% categoricals]

# Categorize m/feth
#meth.categories = c(rep('Caucasian', 6), 'Mexican', 'African-American', 'Asian', 'Mixed', 'Other')
meth.categories = c(rep('Caucasian', 6), 'Other', 'African-American', 'Other', 'Other', 'Other')
for (i in 1:length(meth.categories)) {
  births.raw$meth[births.raw$meth == i-1] = meth.categories[i]
  births.raw$feth[births.raw$feth == i-1] = meth.categories[i]
}
births.raw$meth = as.factor(births.raw$meth)
births.raw$feth = as.factor(births.raw$feth)

# Categorize m/fed
#med.categories = c('elementary school', 'middle school', 'high school', 'high school + trade school', 'high school + some college', 'college graduate', 'trade school', 'high school unclear')
med.categories = c('Other', 'middle school', 'high school', 'Other', 'high school + some college', 'college graduate', 'Other', 'Other')
for (i in 1:length(med.categories)) {
  births.raw$med[births.raw$med == i-1] = med.categories[i]
  births.raw$fed[births.raw$fed == i-1] = med.categories[i]
}
births.raw$med = as.factor(births.raw$med)
births.raw$fed = as.factor(births.raw$fed)

# Categorize marital
#marital.categories = c('married', 'legally separated', 'divorced', 'widowed', 'never married')
marital.categories = c('married', 'single', 'single', 'single', 'single')
for (i in 1:length(marital.categories)) {
  births.raw$marital[births.raw$marital == i] = marital.categories[i]
}
births.raw$marital = as.factor(births.raw$marital)

# Categorize income
income.categories = c('under 2500', '2500-4999', '5000-7499', '7500-9999', '10000-12499', '12500-14999', '15000-17499', '17500-19999', '20000-22499', 'over 22500')
for (i in 1:length(income.categories)) {
  births.raw$income[births.raw$income == i-1] = income.categories[i]
}
births.raw$income = as.factor(births.raw$income)

# Categorize smoke
smoke.categories = c('never', 'smokes now', 'until pregnancy', 'used to, not anymore')
for (i in 1:length(smoke.categories)) {
  births.raw$smoke[births.raw$smoke == i-1] = smoke.categories[i]
}
births.raw$smoke = as.factor(births.raw$smoke)

# Categorize time
#time.categories = c('never or more than 10 years', 'still smokes', 'during pregnancy', 'less than a year', '1-2 years', '2-3 years', '3-4 years', '5-9 years', 'never or more than 10 years', 'quit but don’t know when')
time.categories = c('never or more than 10 years', 'still smokes', 'during pregnancy', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'never or more than 10 years', 'quit but don’t know when')
for (i in 1:length(time.categories)) {
  births.raw$time[births.raw$time == i-1] = time.categories[i]
}
births.raw$time = as.factor(births.raw$time)

# Categorize number
#number.categories = c('never smoked', '1-4', '5-9', '10-14', '15-19', '20-29', '30-39', '40-60', 'more than 60', 'smoked but don’t know how much')
number.categories = c('never smoked', 'less than 20', 'less than 20', 'less than 20', 'less than 20', 'more than 20', 'more than 20', 'more than 20', 'more than 20', 'smoked but don’t know how much')
for (i in 1:length(number.categories)) {
  births.raw$number[births.raw$number == i-1] = number.categories[i]
}
births.raw$number = as.factor(births.raw$number)

births.raw$smoke = NULL
summary(births.raw)
```

### Pair plots

```{r}
births.raw.cont = births.raw[ , ! names(births.raw) %in% categoricals]
summary(births.raw.cont)
pairs(births.raw.cont, cex = 0.2)
```

### NA's and $\pm$Inf's

```{r}
proportion <- sapply(births.raw, function(x) { sum(is.na(x))*100/nrow(births.raw) })
proportion

# births.reduced <- births.raw.cont[, !names(births.raw) %in% c('fht', 'fwt')]

births.mice <- mice(births.raw, method = "cart", seed = 1)
births.clean <- complete(births.mice)
summary(births.clean)
anyNA(births.clean)
```

## Automated Model Selection

```{r}
M0 <- lm(wt ~ 1, data = births.clean) # initial model
Mfull <- lm(wt ~ (. - number - time - fed - med - feth - meth - marital)^2 + number + time + fed + med + feth + meth + marital+
              I(gestation^2) + I(parity^2) + I(mage^2) + I(mht^2) + 
              I(mwt^2) + I(fage^2) + I(fht^2) + I(fwt^2), data = births.clean) # full model
Mstart <- lm(wt ~ ., data = births.clean) # start model
```

```{r}
beta.max <- coef(Mfull)
names(beta.max)[is.na(beta.max)]
```

### Forward Model Selection

```{r}
# forward selection
Mfwd <- step(object = M0,
             scope = list(lower = M0, upper = Mfull),
             direction = "forward",
             trace = FALSE)
```

### Backward Model Selection

```{r}
# backward elimination
Mback <- step(object = Mfull, 
              scope = list(lower = M0, upper = Mfull),
              direction = "backward", 
              trace = FALSE)
```

### Stepwise Model Selection

```{r}
# stepwise selection
Mstep <- step(object = Mstart, 
              scope = list(lower = M0, upper = Mfull),
              direction = "both", 
              trace = FALSE)
```

## Manual Model Selection


```{r}
# models to compare
M1 <- Mfwd
M2 <- Mstep
```


### PRESS Statistics

```{r}
#Press Statistic
calPress <- function(M){
  #X <- model.matrix(M)
  #H <- X %*% solve(crossprod(X), t(X))
  #press <- resid(M)/(1-diag(H))
  press <- resid(M)/(1-hatvalues(M))
  press <- sum(press^2)
  return(press)
}

press1 <- calPress(Mfwd);
press2 <- calPress(Mback);
press3 <- calPress(Mstep);
```



### Akaike Information Criterion
```{r}
# AIC Statistic for M1
AIC1 <- AIC(M1)

#AIC Statistic for M2
AIC2 <- AIC(M2)
```



## Conclusion

# Model Diagnostics

## Linear Model Assumptions
```{r}
summary(Mfwd)
summary(Mback)
summary(Mstep)
```

```{r}

# function for drawing studendized residual plots 
StuResPlot <- function(M) {
  cex <- .8
  res <- residuals(M)
  h <- hatvalues(M)  # Hat matrix
  sigma.hat <- sigma(M)
  res.stu <- resid(M)/sqrt(1-h) # studentized residuals, but on the data scale
  y.hat <- predict(M)
  par(mar = c(4,4,.1,.1))
  plot(y.hat, res, pch = 21, bg = "black", cex = cex, cex.axis = cex,
        xlab = "Predicted Birth Weight",
        ylab = "Residual Birth Weight",
        abline(h = mean(res), col = "blue"))
  points(y.hat, res.stu, pch = 21, bg = "red", cex = cex)
  legend(x = "bottomleft", c("Residuals", "Studentized Residuals"),
        pch = 21, pt.bg = c("black", "red"), pt.cex = cex, cex = cex)
}

# function for drawing standardized residual plots
StaResPlot <- function(M) {
  cex <- .8
  res <- residuals(M) # Hat matrix
  h <- hatvalues(M)
  sigma.hat <- sigma(M)
  par(mar = c(4,4,.1,.1))
  hist(res/sigma.hat, breaks = 50, freq = FALSE, cex.axis = cex,
  xlab = "Standardized Residual", main = "")
  curve(dnorm(x), col = "red", add = TRUE)
  abline(v = mean(res/sigma.hat), col = "green")
}

#function for drawing QQ plots
QQPlot <- function(M) {
  res <- residuals(M)
  h <- hatvalues(M)
  qqnorm(res/sigma(M))
  abline(a=0, b=1, col = "red") #add 45 degree line
  # head(res.stu1)
}


# studendized residual for forward selection model:
StuResPlot(Mfwd)

# studendized residual for backward elimination model:
StuResPlot(Mback)

# studentdized residual for stepwise selection model:
StuResPlot(Mstep)

# standardized residual for forward selection model:
StaResPlot(Mfwd)

# standardized residual for backward elimination model:
StaResPlot(Mback)

# standardized residual for stepwise selection model:
StaResPlot(Mstep)

# QQ-plot for forward selection model:
QQPlot(Mfwd)

# QQ-plot for backward selection model:
QQPlot(Mback)

# QQ-plot for stepwise selection model:
QQPlot(Mstep)
```

## Leverage
```{r}
# function to produce leverage and influence plots
LeverageInfluence <- function(M) {
  y.hat <- predict(M) # predicted values
  sigma.hat <- sigma(M)
  res <- resid(M) # original residuals$
  res.sta <- res/sigma.hat # standardized residuals

  # compute leverages
  h <- hatvalues(M) # hat matrix
  res.stu <- res.sta/sqrt(1-h) # studentized residuals
  
  # PRESS residuals
  press <- res/(1-h)
  
  
  # DFFITS residuals
  dfts <- dffits(M)
  
  # standardize each of these such that they are identical at the average
  # leverage value
  p <- length(coef(M))
  n <- nobs(M)
  hbar <- p/n # average leverage
  res.stu <- res.stu*sqrt(1 - hbar) # at h = hbar, res.stu = res.sta
  press <- press*(1 - hbar)/sigma.hat # at h = hbar, press = res.sta
  dfts <- dfts*(1 - hbar)/sqrt(hbar) # at h = hbar, dfts = res.sta
  
  # plot all residuals
  par(mfrow = c(1,2), mar = c(4,4,.1,.1))
  cex <- .5
  plot(y.hat, rep(0, length(y.hat)), 
       type = "n", # empty plot to get the axis range
       ylim = range(res.sta, res.stu, press, dfts), 
       cex.axis = cex, 
       xlab = "Predicted Values", 
       ylab = "Residuals")
  
  # dotted line connecting each observations residuals for better visibility
  segments(x0 = y.hat, 
           y0 = pmin(res.sta, res.stu, press, dfts), 
           y1 = pmax(res.sta, res.stu, press, dfts), 
           lty = 2)
  points(y.hat, res.sta, pch = 21, bg = "black", cex = cex)
  points(y.hat, res.stu, pch = 21, bg = "blue", cex = cex)
  points(y.hat, press, pch = 21, bg = "red", cex = cex)
  points(y.hat, dfts, pch = 21, bg = "orange", cex = cex)
  
  # against leverages
  plot(h, rep(0, length(y.hat)), 
       type = "n", 
       cex.axis = cex,
       ylim = range(res.sta, res.stu, press, dfts), 
       xlab = "Leverages", ylab = "Residuals")
  
  segments(x0 = h,
          y0 = pmin(res.sta, res.stu, press, dfts), 
          y1 = pmax(res.sta, res.stu, press, dfts), 
          lty = 2)
  points(h, res.sta, pch = 21, bg = "black", cex = cex)
  points(h, res.stu, pch = 21, bg = "blue", cex = cex)
  points(h, press, pch = 21, bg = "red", cex = cex)
  points(h, dfts, pch = 21, bg = "orange", cex = cex)
  abline(v = hbar, col = "grey60", lty = 2)
  
  # cook's distance vs. leverage
  D <- cooks.distance(M)
  
  # flag some of the points
  infl.ind <- which.max(D) # top influence point
  lev.ind <- h > 2*hbar # leverage more than 2x the average
  clrs <- rep("black", len = n)
  clrs[lev.ind] <- "blue"
  clrs[infl.ind] <- "red"
  par(mfrow = c(1,1), mar = c(4,4,1,1))
  cex <- .8
  plot(h, D, xlab = "Leverage", ylab = "Cook's Influence Measure",
  pch = 21, bg = clrs, cex = cex, cex.axis = cex)
  p <- length(coef(M))
  n <- nrow(births.clean)
  hbar <- p/n # average leverage
  abline(v = 2*hbar, col = "grey60", lty = 2) # 2x average leverage
  return(c(infl.ind, D[infl.ind]))
}


# leverage influence for forward selection model:
LeverageInfluence(Mfwd)
LeverageInfluence(Mback)
```


## Colinearity

### Variance Inflation Factor
```{r}
M1 = Mfwd
M2 = Mstep
X <- model.matrix(M1)
C <- cor(X) # correlation matrix
vif1 <- diag(solve(C))
vif1
X <- model.matrix(M2)
C <- cor(X) # correlation matrix
vif2 <- diag(solve(C))
vif2
```


## Outliers
```{r}
M <- with(births.clean,expr ={
   log.wt <- log(births.clean$wt)
   log.gestation <- log(births.clean$gestation)
   log.parity <- log(births.clean$parity)
   log.mage <- log(births.clean$mage)
   log.mgt <- log(births.clean$mht)
   log.mwt <- log(births.clean$mwt)
   log.fage <- log(births.clean$fage)
   log.fht <- log(births.clean$fht)
   log.fwt <-log(births.clean$fwt)
   lm(log.wt ~ log.gestation + log.parity + log.mage + log.mgt + log.mwt + log.fage + log.fht + log.fwt + meth + feth + med + fed + marital + income + smoke + time + number)
    } )


coef(M)# coefficients


plot(predict(M),resid(M),
     xlab = "Predicted Log-Infant Mortality",ylab = "Residual Log-Infant Mortality")
```


### Cross Validation
```{r}
options(warn=-1)      #turn off warnings
# Cross-validation setup
nreps <- 2e3 # number of replications
ntot <- nrow(births.clean) # total number of observations
ntrain <- floor(ntot * 0.7) # size of training set
ntest <- ntot-ntrain # size of test set
mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
mspe2 <- rep(NA, nreps)
logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
system.time({
  for(ii in 1:nreps) {
    if(ii%%200 == 0) message("ii = ", ii)
    # randomly select training observations
    train.ind <- sample(ntot, ntrain) # training observations
    # refit the models on the subset of training data; ?update for details!
    M1.cv <- update(M1, subset = train.ind)
    M2.cv <- update(M2, subset = train.ind)
    # out-of-sample residuals for both models
    # that is, testing data - predictions with training parameters
    M1.res <- births.clean$wt[-train.ind] -
              predict(M1.cv, newdata = births.clean[-train.ind,])
    M2.res <- births.clean$wt[-train.ind] -
              predict(M2.cv, newdata = births.clean[-train.ind,])
    # mean-square prediction errors
    mspe1[ii] <- mean(M1.res^2)
    mspe2[ii] <- mean(M2.res^2)
    # out-of-sample likelihood ratio
    M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) # MLE of sigma
    M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
    # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
    logLambda[ii] <- sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log = TRUE))
    logLambda[ii] <- logLambda[ii] -
                     sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log = TRUE))
  }
})
options(warn=1)      #turn warnings back on
```

```{r}
# plot rMSPE and out-of-sample log(Lambda)
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = expression(M[FWD], M[STEP]), cex = .7,
        ylab = expression(sqrt(MSPE)), col = c("yellow", "orange"))
hist(logLambda, breaks = 50, freq = FALSE,
     xlab = expression(Lambda^{test}),
     main = "", cex = .7)
abline(v = mean(logLambda), col = "red") # average value
```


# Conclusions

