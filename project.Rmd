---
title: "STAT 331 Project"
author: "SiJie Jin, Yinong Wang, Raymond Tan"
date: "November 14th, 2018"
output: pdf_document
toc: yes
toc_section: 2
theme: united
highlighted: tango
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mice)
```
\newpage
# Summary


# Model Selection


## Pre-fitting data diagnostic

```{r prefit ,echo = FALSE}
# read in data
births.raw = read.csv("chds_births.csv")
```

### Summary of Data

After loading the dataset, we observed the following:

1. There are 1236 entries in the dataset
2. There are 18 factors in the dataset
3. Some factors have a large proportion of NA's:
    + fht: 39.81%
    + fwt: 40.37%
    + income: 10.03%
4. Factor marital contains undefined level 0

```{r,echo=FALSE}
head(births.raw)
summary(births.raw)

proportion <- sapply(births.raw, function(x) { sum(is.na(x))*100/nrow(births.raw) })
proportion

# Validating Variables
births.raw$marital[!births.raw$marital %in% c(1:5)] = NA
births.raw$time[births.raw$time %in% c(9)] = NA
births.raw$number[births.raw$number %in% c(9)] = NA

```

### Categorize data
By using one hot encoding method, we factorize the levels of each categorical variables into meaningful names to have a good interpretation of the model.

* Note: we also shrink the number of levels for some factors because those levels are significant minorities:
    + meth/feth: Mexican, Asin, and Mixed are significant minorities and are merged into level Other
    + med/fed: elementary school, high school + trade school, trade school, and high school unclear are significant minorities and are merged into level Other
    + marital: legally separated, divorced, widowed, and never married are significant minorities and are merged into level single
    + time: less than a year, 1-2 years, 2-3 years, 3-4 years, 5-9 years are significant minorities and are merged into level less than 10 years; never and more than 10 years are significant minorities and are merged into level never or more than 10 years
    + number: 1-4, 5-9, 10-14, 15-19 are significant minorities and are merged into level less than 20; 20-29, 30-39, 40-60, more than 60 are significant minorities and are merged into level more than 20
* Note: we converted the following factor levels into NA for mice to impute more useful data
    + time: level quit but don't know when is converted into NA
    + number: level smoked but don’t know how much is converted into NA
* Note: factor smoke is highly correlated to factor time and number, so we droped this factor

```{r,echo=FALSE}
categoricals = c('meth', 'feth', 'med', 'fed', 'marital', 'income', 'smoke', 'time', 'number')#, 'parity', 'mht', 'fht')
continuous = names(births.raw)[! names(births.raw) %in% categoricals]

# Categorize m/feth
#meth.categories = c(rep('Caucasian', 6), 'Mexican', 'African-American', 'Asian', 'Mixed', 'Other')
meth.categories = c(rep('Caucasian', 6), 'Other', 'African-American', 'Other', 'Other', 'Other')
for (i in 1:length(meth.categories)) {
  births.raw$meth[births.raw$meth == i-1] = meth.categories[i]
  births.raw$feth[births.raw$feth == i-1] = meth.categories[i]
}
births.raw$meth = as.factor(births.raw$meth)
births.raw$feth = as.factor(births.raw$feth)

# Categorize m/fed
#med.categories = c('elementary school', 'middle school', 'high school', 'high school + trade school', 'high school + some college', 'college graduate', 'trade school', 'high school unclear')
med.categories = c('Other', 'middle school', 'high school', 'Other', 'high school + some college', 'college graduate', 'Other', 'Other')
for (i in 1:length(med.categories)) {
  births.raw$med[births.raw$med == i-1] = med.categories[i]
  births.raw$fed[births.raw$fed == i-1] = med.categories[i]
}
births.raw$med = as.factor(births.raw$med)
births.raw$fed = as.factor(births.raw$fed)

# Categorize marital
#marital.categories = c('married', 'legally separated', 'divorced', 'widowed', 'never married')
marital.categories = c('married', 'single', 'single', 'single', 'single')
for (i in 1:length(marital.categories)) {
  births.raw$marital[births.raw$marital == i] = marital.categories[i]
}
births.raw$marital = as.factor(births.raw$marital)

# Categorize income
income.categories = c('under 2500', '2500-4999', '5000-7499', '7500-9999', '10000-12499', '12500-14999', '15000-17499', '17500-19999', '20000-22499', 'over 22500')
for (i in 1:length(income.categories)) {
  births.raw$income[births.raw$income == i-1] = income.categories[i]
}
births.raw$income = as.factor(births.raw$income)

# Categorize smoke
smoke.categories = c('never', 'smokes now', 'until pregnancy', 'used to, not anymore')
for (i in 1:length(smoke.categories)) {
  births.raw$smoke[births.raw$smoke == i-1] = smoke.categories[i]
}
births.raw$smoke = as.factor(births.raw$smoke)

# Categorize time
#time.categories = c('never or more than 10 years', 'still smokes', 'during pregnancy', 'less than a year', '1-2 years', '2-3 years', '3-4 years', '5-9 years', 'never or more than 10 years', 'quit but don’t know when')
time.categories = c('never or more than 10 years', 'still smokes', 'during pregnancy', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'less than 10 year', 'never or more than 10 years', 'quit but don’t know when')
for (i in 1:length(time.categories)) {
  births.raw$time[births.raw$time == i-1] = time.categories[i]
}
births.raw$time = as.factor(births.raw$time)

# Categorize number
#number.categories = c('never smoked', '1-4', '5-9', '10-14', '15-19', '20-29', '30-39', '40-60', 'more than 60', 'smoked but don’t know how much')
number.categories = c('never smoked', 'less than 20', 'less than 20', 'less than 20', 'less than 20', 'more than 20', 'more than 20', 'more than 20', 'more than 20', 'smoked but don’t know how much')
for (i in 1:length(number.categories)) {
  births.raw$number[births.raw$number == i-1] = number.categories[i]
}
births.raw$number = as.factor(births.raw$number)

births.raw$smoke = NULL
# summary(births.raw)
```

### Pair plots

```{r,echo=FALSE}
births.raw.cont = births.raw[ , ! names(births.raw) %in% categoricals]
# summary(births.raw.cont)
pairs(births.raw.cont, cex = 0.2)
```
Here is the pair plots of continuous variables. Notice that $parity$ is not continuous because there are some vertical lines in the plot. Also $parity$ is the total number of previous pregnancies, so normally it does not contain decimal numbers. What we did was categorizing it into 6 different variables.

```{r,echo=TRUE}
table(births.raw$parity)

# Categorize parity
births.raw$parity[births.raw$parity > 5] = 5
parity.categories = c('zero', 'one', 'two', 'three', 'four', 'more than four')
for (i in 1:length(parity.categories)) {
  births.raw$parity[births.raw$parity == i-1] = parity.categories[i]
}
births.raw$parity = as.factor(births.raw$parity)
```


### NA's and $\pm$Inf's
For choosing the best 2 canadiate models, we want AIC and Press to be as small as possible and $R^2$ to be as large as possible when comparing. From the data above, we can conclude that $fht, fwt, marital, number, time$ and $income$ are missing covariates since these variables take up to a large proportion over 10% of our dataset, which might give us undsired results when modelling these data. Since throwing out more than 10% of our data could cause misleading to our analysis. We would compelete these covariates by using Imputatation 5 times via the mice package.


```{r,echo=FALSE,warning=FALSE}
invisible(capture.output(births.mice <- mice(births.raw, method = "cart", seed = 1)))
births.clean <- complete(births.mice)
# summary(births.clean)
# anyNA(births.clean)
```

## Pre-fitting Model

```{r,echo=FALSE,waring=FALSE}
prefit.model <- lm(wt ~ gestation + mage + mwt + fage + fwt, data = births.clean)
```

## Automated Model Selection

```{r,echo=FALSE}
M0 <- lm(wt ~ 1, data = births.clean) # initial model
Mfull <- lm(wt ~ (. - number - time - fed - med - feth - meth - marital)^2+ number + time + fed + med + feth                + meth + marital + I(gestation^2) + I(mage^2) + I(mht^2)
              + I(mwt^2) + I(fage^2) + I(fht^2) + I(fwt^2), data = births.clean) # full model
Mstart <- lm(wt ~ ., data = births.clean) # start model
```

```{r,echo=FALSE,eval=FALSE}
beta.max <- coef(Mfull)
names(beta.max)[is.na(beta.max)]
```

```{r,echo=FALSE}
# forward selection
Mfwd <- step(object = M0,
             scope = list(lower = M0, upper = Mfull),
             direction = "forward",
             trace = FALSE)
```


```{r,echo=FALSE}
# backward elimination
Mback <- step(object = Mfull,
              scope = list(lower = M0, upper = Mfull),
              direction = "backward",
              trace = FALSE)
```

```{r,echo=FALSE}
# stepwise selection
Mstep <- step(object = Mstart,
              scope = list(lower = M0, upper = Mfull),
              direction = "both",
              trace = FALSE)
```

## Manual Model Selection

```{r,echo=FALSE,eval=FALSE}
summary(Mfwd)
summary(Mback)
summary(Mstep)
summary(prefit.model)
```

### PRESS, $R^2$ and AIC

```{r,echo=FALSE}
#Press Statistic
calPress <- function(M){
  #X <- model.matrix(M)
  #H <- X %*% solve(crossprod(X), t(X))
  #press <- resid(M)/(1-diag(H))
  press <- resid(M)/(1-hatvalues(M))
  press <- sum(press^2)
  return(press)
}

press1 <- calPress(Mfwd);
press2 <- calPress(Mback);
press3 <- calPress(Mstep);
press4 <- calPress(prefit.model)

presses <- c(press1, press2, press3, press4)

# AIC Statistic
AIC1 <- AIC(Mfwd)
AIC2 <- AIC(Mback)
AIC3 <- AIC(Mstep)
AIC4 <- AIC(prefit.model)

AICS <- c(AIC1, AIC2, AIC3, AIC4)


```


```{r,echo=FALSE}
r.squared <- c(summary(Mfwd)$r.squared, summary(Mback)$r.square, 
               summary(Mstep)$r.square, summary(prefit.model)$r.square)
info <- c(r.squared,AICS,presses)
info <- matrix(t(info),nrow = 4,ncol = 3)
rownames(info)<-c("MFwd", "MBack", "MStep", "MPrefit")
colnames(info)<-c("R^2", "AIC", "Press")
info  

```

We got 3 different models, $MFwd$, $MBack$, $MStep$, by performing forward selection, backward elimination and stepwise selection on full model respectively. From the above table generated by R, we can see that $MBack$ has the smallest AIC Press Statistic and has the largest $R^2$ compared to the other two models. By comparing $MStep$ and $MFwd$, $MStep$ seems to have a better $R^2$ and a better AIC than $MFwd$, but $MFwd$ only has a better PRESS Statistic than $MStep$. Therefore, $Mback \cap MStep \in$ our canadiate set. 
We also got a pre-fitting model $MPrefit$ by analyzing pair plots of continuous variable. We noticed that there is a linear relation between $mht$ and $mwt$ and there also exists a linear relation between $fht$ and $fwt$. Hence in the pre-fitting model, we removed the $mht$ and $fht$. Since $MPrefit$ has least $R^2$ and largest AIC and PRESS Statistic, We do not choose this model as our canadiate model.

```{r,echo=FALSE}
# models to compare
M1 <- Mback
M2 <- Mstep
```

# Model Diagnostics

## Linear Model Assumptions

Since our canadiate set only has $MBack$ and $MStep$, we only need to generate plots,Studentized Residuals VS Fitted, Standardized Histogram and QQ plot, for these individuals.

```{r,echo=FALSE}

# function for drawing studendized residual plots
StuResPlot <- function(M,name) {
  cex <- .8
  res <- residuals(M)
  h <- hatvalues(M)  # Hat matrix
  sigma.hat <- sigma(M)
  res.stu <- resid(M)/sqrt(1-h) # studentized residuals, but on the data scale
  y.hat <- predict(M)
 
  par(4,.1,4,.1)
  plot(y.hat, res, pch = 21, bg = "black", cex = cex, cex.axis = cex,
        xlab = "Predicted Birth Weight",
        ylab = "Residual Birth Weight",
        main = paste("Studentized Residuals for",name),
        abline(h = mean(res), col = "blue"))
  points(y.hat, res.stu, pch = 21, bg = "red", cex = cex)
  legend(x = "bottomleft", c("Residuals", "Studentized Residuals"),
        pch = 21, pt.bg = c("black", "red"), pt.cex = cex, cex = cex)
}

# function for drawing standardized residual plots
StaResPlot <- function(M,name) {
  cex <- .8
  res <- residuals(M) # Hat matrix
  h <- hatvalues(M)
  sigma.hat <- sigma(M)
  # par(mar = c(4,2,.1,2))
  par(4,.1,4,.1)
  hist(res/sigma.hat, breaks = 50, freq = FALSE, cex.axis = cex,
  xlab = "Standardized Residual", main = paste("Standardize Residuals for",name))
  curve(dnorm(x), col = "red", add = TRUE)
  abline(v = mean(res/sigma.hat), col = "green")
}

#function for drawing QQ plots
QQPlot <- function(M,name) {
  res <- residuals(M)
  h <- hatvalues(M)
  qqnorm(res/sigma(M),main = paste("QQ Plot for",name))
  abline(a=0, b=1, col = "red") #add 45 degree line
  # head(res.stu1)
}


```



```{r,echo=FALSE}

 par(mfrow = c(1,2))
# studendized residual for backward elimination model:
StuResPlot(Mback,"Mback")

# studentdized residual for stepwise selection model:
StuResPlot(Mstep,"MStep")

```

To begin with, we compare Studentized Residuals plot between the two canadiates. These two plots are very similar in a way such that their mean at zero(shown by a blue horizontal line), they do not display any linear/cureventure trends, which impiles there they are independence, they both have a constant variance since the error variance does not have a increasing/decresing trend with the independent varaibles.



```{r,echo=FALSE}
par(mfrow = c(1,2))
# standardized residual for backward elimination model:
StaResPlot(Mback,"MBack")

# standardized residual for stepwise selection model:
StaResPlot(Mstep,"MStep")
```


Next, we would look at the Standardized Historgram to find the assumption of mormality and to find any abnormal residuals about the rwo models. From the graphs, $Mstep$ has a slightly better normality than $MBack$. As the density of $Mstep$ touches the normal curve in range of [[-3,-1],[1,3]], whereas the density of $MBack$ touches the normal curve in the range of [[-2,-3],[2,3]]. In additon, we can clearly see that there is no standardized residuals lies outside of [-3,3], which impiles there is no abnormal residuals for both models.

```{r,echo=FALSE}

 par(mfrow = c(1,2))
# QQ-plot for backward selection model:
QQPlot(Mback,"MBack")

# QQ-plot for stepwise selection model:
QQPlot(Mstep,"MStep")
```

Last but not least, We compare their QQ-Plots. From both QQ-Plots, as theroretical quantiles getting larger, each points tend to stabalize on the 45 degree line on the plots.\newline

In Conclution, we can not determine which model is better than the other from a linear assumptions point of view since the evidences from above can not distinguish the advantage of the two. 

## Leverage
```{r,echo=FALSE}
# function to produce leverage and influence plots
LeverageInfluence <- function(M) {
  y.hat <- predict(M) # predicted values
  sigma.hat <- sigma(M)
  res <- resid(M) # original residuals$
  res.sta <- res/sigma.hat # standardized residuals

  # compute leverages
  h <- hatvalues(M) # hat matrix
  res.stu <- res.sta/sqrt(1-h) # studentized residuals

  # PRESS residuals
  press <- res/(1-h)

  # DFFITS residuals
  dfts <- dffits(M)

  # standardize each of these such that they are identical at the average
  # leverage value
  p <- length(coef(M))
  n <- nobs(M)
  hbar <- p/n # average leverage
  res.stu <- res.stu*sqrt(1 - hbar) # at h = hbar, res.stu = res.sta
  press <- press*(1 - hbar)/sigma.hat # at h = hbar, press = res.sta
  dfts <- dfts*(1 - hbar)/sqrt(hbar) # at h = hbar, dfts = res.sta

  # plot all residuals
  par(mfrow = c(1,3), mar = c(4,4,.1,1.1))
  cex <- .5
  plot(y.hat, rep(0, length(y.hat)),
       type = "n", # empty plot to get the axis range
       ylim = range(res.sta, res.stu, press, dfts),
       cex.axis = cex,
       xlab = "Predicted Values",
       ylab = "Residuals")

  # dotted line connecting each observations residuals for better visibility
  segments(x0 = y.hat,
           y0 = pmin(res.sta, res.stu, press, dfts),
           y1 = pmax(res.sta, res.stu, press, dfts),
           lty = 2)
  points(y.hat, res.sta, pch = 21, bg = "black", cex = cex)
  points(y.hat, res.stu, pch = 21, bg = "blue", cex = cex)
  points(y.hat, press, pch = 21, bg = "red", cex = cex)
  points(y.hat, dfts, pch = 21, bg = "orange", cex = cex)

  # against leverages
  plot(h, rep(0, length(y.hat)),
       type = "n",
       cex.axis = cex,
       ylim = range(res.sta, res.stu, press, dfts),
       xlab = "Leverages", ylab = "Residuals")

  segments(x0 = h,
          y0 = pmin(res.sta, res.stu, press, dfts),
          y1 = pmax(res.sta, res.stu, press, dfts),
          lty = 2)
  points(h, res.sta, pch = 21, bg = "black", cex = cex)
  points(h, res.stu, pch = 21, bg = "blue", cex = cex)
  points(h, press, pch = 21, bg = "red", cex = cex)
  points(h, dfts, pch = 21, bg = "orange", cex = cex)
  abline(v = hbar, col = "grey60", lty = 2)

  # cook's distance vs. leverage
  D <- cooks.distance(M)

  # flag some of the points
  infl.ind <- which.max(D) # top influence point
  lev.ind <- h > 2*hbar # leverage more than 2x the average
  clrs <- rep("black", len = n)
  clrs[lev.ind] <- "blue"
  clrs[infl.ind] <- "red"
  # par(mfrow = c(1,1), mar = c(4,4,1,1))
  cex <- .8
  plot(h, D, xlab = "Leverage", ylab = "Cook's Influence Measure",
  pch = 21, bg = clrs, cex = cex, cex.axis = cex)
  p <- length(coef(M))
  n <- nrow(births.clean)
  hbar <- p/n # average leverage
  abline(v = 2*hbar, col = "grey60", lty = 2) # 2x average leverage
  legend("topleft", legend = c("High Leverage", "High Influence"), pch = 21,
         pt.bg = c("blue", "red"), cex = cex, pt.cex = cex)
  return(c(infl.ind, D[infl.ind]))
}


# leverage influence for forward selection model:
a <-LeverageInfluence(Mback)
b <-LeverageInfluence(Mstep)

```

## Colinearity

## Outliers
```{r,echo=FALSE}
M <-lm(births.clean)
# coef(M)
plot(predict(M),resid(M))
entry <- which.max(resid(M))
entry
```


### Cross Validation
```{r,echo=FALSE}
options(warn=-1)      #turn off warnings
# Cross-validation setup
nreps <- 2e3 # number of replications
ntot <- nrow(births.clean) # total number of observations
ntrain <- floor(ntot * 0.7) # size of training set
ntest <- ntot-ntrain # size of test set
mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
mspe2 <- rep(NA, nreps)
logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
system.time({
  for(ii in 1:nreps) {
    if(ii%%200 == 0) message("ii = ", ii)
    # randomly select training observations
    train.ind <- sample(ntot, ntrain) # training observations
    # refit the models on the subset of training data; ?update for details!
    M1.cv <- update(M1, subset = train.ind)
    M2.cv <- update(M2, subset = train.ind)
    # out-of-sample residuals for both models
    # that is, testing data - predictions with training parameters
    M1.res <- births.clean$wt[-train.ind] -
              predict(M1.cv, newdata = births.clean[-train.ind,])
    M2.res <- births.clean$wt[-train.ind] -
              predict(M2.cv, newdata = births.clean[-train.ind,])
    # mean-square prediction errors
    mspe1[ii] <- mean(M1.res^2)
    mspe2[ii] <- mean(M2.res^2)
    # out-of-sample likelihood ratio
    M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) # MLE of sigma
    M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
    # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
    logLambda[ii] <- sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log = TRUE))
    logLambda[ii] <- logLambda[ii] -
                     sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log = TRUE))
  }
})
options(warn=1)      #turn warnings back on
```

```{r,echo=FALSE}
# plot rMSPE and out-of-sample log(Lambda)
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = expression(M[BACK], M[STEP]), cex = .7,
        ylab = expression(sqrt(MSPE)), col = c("yellow", "orange"))
hist(logLambda, breaks = 50, freq = FALSE,
     xlab = expression(Lambda^{test}),
     main = "", cex = .7)
abline(v = mean(logLambda), col = "red") # average value


# r.squared <- c(summary(M1)$r.squared,summary(M2)$r.squared)
# 
# 
# presses <- c(calPress(M1), calPress(M2))
# 
# 
# AICS <-c(AIC(M1),AIC(M2))
# 
# info <-matrix(c(r.squared,presses,AICS),nrow = 2, ncol = 3)
# rownames(info) <- c("MBack","MStep")
# colnames(info)<- c("R^2","Press","AIC")
# 
# info
# 

```


# Conclusions



\newpage
#Appendix for R Code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```




