†---
title: "STAT331 Project"
author: "Sijie Jin, Raymond Tan, Yinong Wang"
date: '2018-11-14'
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

# Model Selection

## Pre-fitting data diagnostic

```{r}
library(mice)

# read in data
births.raw = read.csv("chds_births.csv")
```

### Summary of Data

```{r}
head(births.raw, 10)
summary(births.raw)

# Validating marital
births.raw$marital[!births.raw$marital %in% c(1:5)] = NA
```

### Categorize data

```{r}
categoricals = c('meth', 'feth', 'med', 'fed', 'marital', 'income', 'smoke', 'time', 'number')#, 'parity', 'mht', 'fht')
continuous = names(births.raw)[! names(births.raw) %in% categoricals]

# Categorize meth
meth.categories = c(rep('Caucasian', 6), 'Mexican', 'African-American', 'Asian', 'Mixed', 'Other')
for (i in 1:length(meth.categories)) {
  births.raw$meth[births.raw$meth == i-1] = meth.categories[i]
  births.raw$feth[births.raw$feth == i-1] = meth.categories[i]
}
births.raw$meth = as.factor(births.raw$meth)
births.raw$feth = as.factor(births.raw$feth)

# Categorize med
med.categories = c('elementary school', 'middle school', 'high school', 'high school + trade school', 'high school + some college', 'college graduate', 'trade school', 'high school unclear')
for (i in 1:length(med.categories)) {
  births.raw$med[births.raw$med == i-1] = med.categories[i]
  births.raw$fed[births.raw$fed == i-1] = med.categories[i]
}
births.raw$med = as.factor(births.raw$med)
births.raw$fed = as.factor(births.raw$fed)

# Categorize marital
marital.categories = c('married', 'legally separated', 'divorced', 'widowed', 'never married')
for (i in 1:length(marital.categories)) {
  births.raw$marital[births.raw$marital == i] = marital.categories[i]
}
births.raw$marital = as.factor(births.raw$marital)

# Categorize income
income.categories = c('under 2500', '2500-4999', '5000-7499', '7500-9999', '10000-12499', '12500-14999', '15000-17499', '17500-19999', '20000-22499', 'over 22500')
for (i in 1:length(income.categories)) {
  births.raw$income[births.raw$income == i-1] = income.categories[i]
}
births.raw$income = as.factor(births.raw$income)

# Categorize smoke
smoke.categories = c('never', 'smokes now', 'until pregnancy', 'used to, not anymore')
for (i in 1:length(smoke.categories)) {
  births.raw$smoke[births.raw$smoke == i-1] = smoke.categories[i]
}
births.raw$smoke = as.factor(births.raw$smoke)

# Categorize time
time.categories = c('never smoked', 'still smokes', 'during pregnancy', 'less than a year', '1-2 years', '2-3 years', '3-4 years', '5-9 years', 'more than 10 years', 'quit but don’t know when')
for (i in 1:length(time.categories)) {
  births.raw$time[births.raw$time == i-1] = time.categories[i]
}
births.raw$time = as.factor(births.raw$time)

# Categorize number
number.categories = c('never smoked', '1-4', '5-9', '10-14', '15-19', '20-29', '30-39', '40-60', 'more than 60', 'smoked but don’t know how much')
for (i in 1:length(number.categories)) {
  births.raw$number[births.raw$number == i-1] = number.categories[i]
}
births.raw$number = as.factor(births.raw$number)

births.raw
```

### Pair plots

```{r}
births.raw.cont = births.raw[ , ! names(births.raw) %in% categoricals]
summary(births.raw.cont)
pairs(births.raw.cont, cex = 0.2)
```

### NA's and $\pm$Inf's

```{r}
proportion <- sapply(births.raw, function(x) { sum(is.na(x))*100/nrow(births.raw) })
proportion

# births.reduced <- births.raw.cont[, !names(births.raw) %in% c('fht', 'fwt')]

births.mice <- mice(births.raw, method = "cart", seed = 1)
births.clean <- complete(births.mice)
summary(births.clean)
anyNA(births.clean)
```

## Automated Model Selection

```{r}
M0 <- lm(wt ~ 1, data = births.clean) # initial model
Mfull <- lm(wt ~ (. - number - time - fed - med - feth - meth - marital)^2 + number + time + fed + med + feth + meth + marital+
              I(gestation^2) + I(parity^2) + I(mage^2) + I(mht^2) + 
              I(mwt^2) + I(fage^2) + I(fht^2) + I(fwt^2), data = births.clean) # full model
Mstart <- lm(wt ~ ., data = births.clean) # start model
```

### Forward Model Selection

```{r}
# forward selection
Mfwd <- step(object = M0,
             scope = list(lower = M0, upper = Mfull),
             direction = "forward",
             trace = FALSE)
```

### Backward Model Selection

```{r}
# backward elimination
Mback <- step(object = Mfull, 
              scope = list(lower = M0, upper = Mfull),
              direction = "backward", 
              trace = FALSE)
```

### Stepwise Model Selection

```{r}
# stepwise selection
Mstep <- step(object = Mstart, 
              scope = list(lower = M0, upper = Mfull),
              direction = "both", 
              trace = FALSE)
```

## Manual Model Selection

### Cross Validation

```{r}
# models to compare
M1 <- Mfwd
M2 <- Mstep
# Cross-validation setup
nreps <- 2e3 # number of replications
ntot <- nrow(births.clean) # total number of observations
ntrain <- floor(ntot * 0.7) # size of training set
ntest <- ntot-ntrain # size of test set
mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
mspe2 <- rep(NA, nreps)
logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
system.time({
  for(ii in 1:nreps) {
    if(ii%%200 == 0) message("ii = ", ii)
    # randomly select training observations
    train.ind <- sample(ntot, ntrain) # training observations
    # refit the models on the subset of training data; ?update for details!
    M1.cv <- update(M1, subset = train.ind)
    M2.cv <- update(M2, subset = train.ind)
    # out-of-sample residuals for both models
    # that is, testing data - predictions with training parameters
    M1.res <- births.clean$wt[-train.ind] -
    predict(M1.cv, newdata = births.clean[-train.ind,])
    M2.res <- births.clean$wt[-train.ind] -
    predict(M2.cv, newdata = births.clean[-train.ind,])
    # mean-square prediction errors
    mspe1[ii] <- mean(M1.res^2)
    mspe2[ii] <- mean(M2.res^2)
    # out-of-sample likelihood ratio
    M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) # MLE of sigma
    M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
    # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
    logLambda[ii] <- sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log = TRUE))
    logLambda[ii] <- logLambda[ii] -
                     sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log = TRUE))
  }
})
```

```{r}
# plot rMSPE and out-of-sample log(Lambda)
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = expression(M[FWD], M[STEP]), cex = .7,
        ylab = expression(sqrt(MSPE)), col = c("yellow", "orange"))
hist(logLambda, breaks = 50, freq = FALSE,
     xlab = expression(Lambda^{test}),
     main = "", cex = .7)
abline(v = mean(logLambda), col = "red") # average value
```

### PRESS Statistics

```{r}
#Press Statistic
calPress <- function(M){
  press <- resid(M)/(1-hatvalues(M))
  press <- sum(press^2)
  return(press)
}

press1 <- calPress(M1);
press2 <- calPress(M2);
```



### Akaike Information Criterion
```{r}
# AIC Statistic for M1
AIC1 <- AIC(M1)

#AIC Statistic for M2
AIC2 <- AIC(M2)
```



## Conclusion

# Model Diagnostics

## Linear Model Assumptions
```{r}
summary(Mfwd)
summary(Mback)
summary(Mstep)
```

```{r}

# function for drawing studendized residual plots 
StuResPlot <- function(M) {
  cex <- .8
  res <- residuals(M) # Hat matrix
  h <- hatvalues(M)
  sigma.hat <- sigma(M)
  res.stu <- res/sqrt(1-h) # studentized residuals, but on the data scale
  y.hat <- predict(M)
  par(mar = c(4,4,.1,.1))
  plot(y.hat, res, pch = 21, bg = "black", cex = cex, cex.axis = cex,
        xlab = "Predicted Birth Weight",
        ylab = "Residual Birth Weight",
        abline(h = mean(res), col = "blue"))
  points(y.hat, res.stu, pch = 21, bg = "red", cex = cex)
  legend(x = "bottomleft", c("Residuals", "Studentized Residuals"),
        pch = 21, pt.bg = c("black", "red"), pt.cex = cex, cex = cex)
}

# function for drawing standardized residual plots
StaResPlot <- function(M) {
  cex <- .8
  res <- residuals(M) # Hat matrix
  h <- hatvalues(M)
  sigma.hat <- sigma(M)
  par(mar = c(4,4,.1,.1))
  hist(res/sigma.hat, breaks = 50, freq = FALSE, cex.axis = cex,
  xlab = "Standardized Residual", main = "")
  curve(dnorm(x), col = "red", add = TRUE)
  abline(v = mean(res/sigma.hat), col = "green")
}

#function for drawing QQ plots
QQPlot <- function(M) {
  res <- residuals(M)
  h <- hatvalues(M)
  res.stu <- res/sqrt(1-h)
  qqnorm(res.stu, ylim = c(-60, 60))
  qqline(res.stu)
  # head(res.stu1)
}


# studendized residual for forward selection model:
StuResPlot(Mfwd)

# studendized residual for backward elimination model:
StuResPlot(Mback)

# studentdized residual for stepwise selection model:
StuResPlot(Mstep)

# standardized residual for forward selection model:
StaResPlot(Mfwd)

# standardized residual for backward elimination model:
StaResPlot(Mback)

# standardized residual for stepwise selection model:
StaResPlot(Mstep)

# QQ-plot for forward selection model:
QQPlot(Mfwd)

# QQ-plot for backward selection model:
QQPlot(Mback)

# QQ-plot for stepwise selection model:
QQPlot(Mstep)
```

## Colinearity

## Outliers

# Conclusions

